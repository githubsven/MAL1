\documentclass[12pt]{article}
\usepackage{hyperref}
\usepackage[affil-it]{authblk}
\usepackage{etoolbox}
\usepackage{lmodern}

\usepackage{amsmath}            % Fancy Mathematical symbols
\usepackage{hyperref}						% Hyperlinks
\usepackage{graphicx}


\makeatletter
\patchcmd{\@maketitle}{\LARGE \@author}{\fontsize{12}{12}\selectfont\@author}{}{}
\makeatother
\renewcommand\Authfont{\fontsize{13}{14.4}\selectfont}
\renewcommand\Affilfont{\fontsize{9}{10.8}\itshape}

\title{Practical 1 - Reinforcement Learning \\
	\large Multi-agent learning 2017-2018}
\author{Sven Luehof ($4235991$) & Michiel van Heusden ($4173309$)}
\date{\today}
\begin{document}
\maketitle

\section*{Problem 1}\label{sec:p1}

\begin{enumerate}
	\item[a)] 						% TODO: uitwerken
	There are a few advantages for using a learning parameter $\alpha$ instead of averaging past rewards.
	The main advantage is that the agent will continue learning from new rewards.
	Assume an $\alpha = 0.05$, then each new value will count for $5\%$ towards the new believed value of the action.
	As $\alpha$ never changes value, old values become less important, with a rate of $(1-\alpha)^{n}$.

	Another advantage can be seen in a system like the one provided for this practical:
	the rate at which strategies converge is much higher with an $\alpha$ value compared to averaging reward values.
	The system as provided, written by Maaike Burghorn and Wouter Something, was used to test how well this holds. %TODO

	In order to do so a tests was dsigned:
	the program that was used to model the system was the practical provided \footnote{\hyperref{}}. % TODO: download link
	The algorithm used was the \emph{Optimistic Starting Value} algorithm, with the following parameters:
	\begin{itemize}
		\item nrAgents $=10$
		\item nrMachines $=4$
		\item initialValues $=5$
	\end{itemize}
	Whereas the $a$ parameter varied ($\alpha = 0.05$, $0.10$, $0.25$, $0.50$, $0.75$, $1.00$).
	A separate test was run without a learning parameter, but with averaging the rewards.

	This setup was used to determine after how many \emph{ticks} (actions) the distribution of agents per machine converged.
	The reason this was chosen to indicate the rate of convergence of the expected reward values is because agents choose the optimal value when using the optimistic starting value algorithm.
	Whenever the amount of agents that are on a certain machine does not change, one could assume that a strategy, and thus the expected rewards, had converged.

	We found that with a learning parameter the system converged within a few hundred ticks.
	The average convergence rates and their standard deviations can be found in table \ref{table:Ex1-TestSummaries}.
	Unfortunately there never was true convergence when running the averaging algorithm; we had to implement an upper limit of ticks.
	We also found that having a higher $\alpha$ was correlated to a faster convergence as can be seen in figure \ref{fig:box-plots-alphavalues}.
	The complete test data can be found in the additional files, which will be provided upon request\footnote{Send an email to \href{mailto:m.p.a.vanheusden@uu.nl}{m.p.a.vanheusden@uu.nl}}.

	A third, and more important advantage to using a learning parameter over averaging rewards can be seen in a non-static environment.
	% TODO: uitwerken

	\begin{table}[]
	\begin{tabular}{l|lllllll}
	Algorithm & $\alpha = 0.05$ & $\alpha = 0.10$ & $\alpha = 0.25$ & $\alpha = 0.50$ & $\alpha = 0.75$ & $\alpha = 1.00$ & Averaging   \\
	\hline
	          & $546.5$     & $312.7$     & $144.2$     & $102.5$     & $78.20$     & $53.50$     & $69.93$       \\
	          & $154.1$     & $86.34$     & $55.65$     & $23.18$     & $35.51$     & $35.12$     & $69.98$
	\end{tabular}
	    \caption{The amount of ticks required until the system converged; parameters: nrAgents $=10$, nrMachines $=3$, initialValues $=5$.}
	    \label{table:Ex1-TestSummaries}
	\end{table}


	% SLides lecture 3, slide 172/174
\end{enumerate}

\section*{Problem 2}
\begin{enumerate}
	\item[a)]
	$\alpha = 0.10$
	nrAgents $= 10$, nrMachines $=3$ and initialValues $=5$
	And the initial value varied ($0$,$1$,$2$,$5$,$10$)

	The complete test data can be found in the added files.

	Optimistic starting values is a greedy search algorithm, therefore we can assume that it eventually converges to a strategy.
	This is because each agent will assign its own expected value to each of the slot machines.
	This value will eventually converge, as shown in exercise $1$.
	Therefore a strategy is bound to converge, as each agent chooses its own optimal machine.

	We, however, do assume that the starting values will have some impact.
	Whenever the starting values are $0$ each agent will pick a single machine and assign a value to it.
	Since the expected rewards of the other machines is $0$, the first selected one will always be the one with the highest reward (assuming no negative values will be rewarded).
	If the starting value is higher than the true lowest expected value, the action distribution will always converge as well.
	This is due to the fact that the expected payoff value of each machine converges as well.

	Why? Since the initial value is the same a random machine will be chosen.
	If the chosen machine is the one with the lowest true expected value, it will eventually no longer be the optimal machine to play.
	If the first machine chosen was not the one with the lowest true expected value, the agent will assign a higher value to that machine than to the other machines.
	This results in a convergence in strategy for the agent.
	If the initial values is higher than the highest true expected value of all machines, this results in each machine being tested for their values.
	The algorithm will choose one random, assign a value lower than the initial value to it.
	This process is then repeated until the expected values of the machines converge and an optimal strategy emerges.

	However, the more expected values that have to converge, the longer it takes to converge.
	Therefore one can assume that a lower initial value corresponds with a shorter time before the system converges.

	In order to prove this a test environment similar to the one used for exercise 1 was created.
	The program, as provided, was used again.
	Once again the optimistic starting value algorithm was used for the tests.
	The constant settings were as follows:
		\begin{itemize}
		\item nrAgents $=10$
		\item nrMachines $=4$
		\item $\alpha = 0.10$
	\end{itemize}
	The initial values were varied (initial values = $0$, $1$, $2$, $5$, $10$).
	Once again the test was conducted $100$ times per initial value.

	The results were as expected: on average the tests with a lower initial value converged faster than the ones with a higher average value.
	This can be seen in \ref{table:Ex2-TestSummaries}.
	However, for starting values higher than $1$ the difference is not that significant, as can be derived from figure \ref{fig:Boxplot-InitialValues}.
	The full test data will be given on request \footnote{Send an email to \href{mailto:m.p.a.vanheusden@uu.nl}{m.p.a.vanheusden@uu.nl}}.


	\begin{table}[]
    \centering
	\begin{tabular}{l|lllll}
	    Initial Values     & $0$ 		& $1$ 	& $2$ 		& $5$ 		& $10$ \\
	    \hline
	    Mean               & $2.150$	& $250.7$	& $271.1$   & $293.7$   & $316.1$         \\
	    Standard Deviation & $0.592$	& $87.55$	& $91.43$   & $80.87$   & $90.99$
	\end{tabular}
	\caption{The amount of ticks required until the system converged; parameters: nrAgents $=10$, nrMachines $=3$, $\alpha = 0.10$.}
	\label{table:Ex2-TestSummaries}
	\end{table}



	\item[b)]
\end{enumerate}

\section*{Problem 3}

\begin{enumerate}
	\item[a)]

	To test whether this algorithm converges the action distribution we ran the algorithm with parameters: $\alpha = 0.10$,
	nrAgents $= 10$, nrMachines $=3$, initialValues $=5$,
	and the $\epsilon$ varied ($0$,$1$,$2$,$5$,$10$).

	After running the algorithm it became apparent that the action distribution converges to a certain slot machine. After executing the $\epsilon$-greedy algorithm for a while, the agents will most often choose the slot machine which results in the highest expected reward. However, the agents will still try other slot machines with probability epsilon, since the algorithm does not diminish the epsilon value.

    The algorithm converges the action distribution, because if performed for an infinite amount of time, all agents will have tried every slot machine and all expected rewards will have converged towards their true reward. It will then decide to use the slot machine with the highest expected reward, because of the algorithm`s greedy property. However, as said before with probability epsilon agents will still sometimes diverge from the optimal slot machine.

    The $\epsilon$ parameter influences the `willingness to explore` of the agents. If the $\epsilon$ parameter is high, the agents will decide to try a different slot machine than the optimal slot machine more often. When epsilon is low, the agents will choose the current optimal slot machine more often.


	\item[b)]

	The disadvantage of a high exploration rate is that the agent will often choose a slot machine with a lower than optimal expected reward. This may result in the finding of a better than previous optimal slot machine, but may also result in a lower achieved reward than if the agent had chosen the optimal slot machine.

    A possible solution for this problem is through diminishing the exploration rate as the algorithm goes through its cycles. At the start of the run you want the agents to have a relatively high exploration rate, to enable them to find the optimal slot machine. However, after it has become likely that they have already found the optimal slot machine, you want to decrease the exploration rate, such that they will choose the optimal slot machine more often. It might even be useful to eventually let the exploration rate become 0, so that the agents from that moment will only choose the slot machine resulting in the highest reward.

\end{enumerate}

\section*{Problem 4}

\begin{enumerate}
	\item[a)]For the softmax algorithm we used parameters $\alpha = 0.10$,
	nrAgents $= 10$, nrMachines $=3$ and the $\tau$ took values of ($0.05$, $0.1$,$0.2$,$0.5$,$1$).

	We notice that the action distribution barely converges. Using a low value for $\tau$ yielded results showing a pattern that suggests convergences, namely after a while the agents seemed to mostly go for a single slot machine, very seldom diverging from this strategy. However, as $\tau$ increases, the convergence of the action distribution seems to disappear - the amount of agents choosing a single slot machine decreases.

	This effect occurs because $\tau$ influences how much the expected reward influences the probability that a certain action is chosen. A high $\tau$ value means that the expected reward has barely no effect on the probability. This principle also applies the other way around, if the $\tau$ is low, the expected reward has more impact on the probability.

	\item[b)]
	Softmax exploit opties met een hoge expected reward meer (wanneer het besluit te searchen. epsilon greedy gaat random exploren.
\end{enumerate}

\section*{Problem 5}



% Extra sources:
\section{Sources}
http://users.isr.ist.utl.pt/~mtjspaan/readingGroup/ProofQlearning.pdf


\end{document}
